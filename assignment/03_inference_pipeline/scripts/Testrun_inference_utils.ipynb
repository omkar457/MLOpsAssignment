{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d23cfcc8-7fe4-4bae-a80a-048693c62536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81541f63-2249-47ff-8667-0f415bb2b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import *\n",
    "import importlib.util"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e7e261f-35f6-4e2a-a49b-83ee7970bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_cleaning_constants = module_from_file('data_cleaning_constants', '/home/assignment/01_data_pipeline/scripts/constants.py')\n",
    "city_tier_mapping = module_from_file('city_tier_mapping', '/home/assignment/01_data_pipeline/scripts/mapping/city_tier_mapping.py')\n",
    "significant_categorical_level = module_from_file('significant_categorical_level', '/home/assignment/01_data_pipeline/scripts/mapping/significant_categorical_level.py')\n",
    "#interaction_mapping = module_from_file('interaction_mapping', '/home/assignment/01_data_pipeline/scripts/mapping/interaction_mapping.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abbf809c-7cee-46f9-b19f-9fa36fe27776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_into_db():\n",
    "    '''\n",
    "    Thie function loads the data present in data directory into the db\n",
    "    which was created previously.\n",
    "    It also replaces any null values present in 'toal_leads_dropped' and\n",
    "    'referred_lead' columns with 0.\n",
    "\n",
    "\n",
    "    INPUTS\n",
    "        DB_FILE_NAME : Name of the database file\n",
    "        DB_PATH : path where the db file should be\n",
    "        DATA_DIRECTORY : path of the directory where 'leadscoring.csv' \n",
    "                        file is present\n",
    "        \n",
    "\n",
    "    OUTPUT\n",
    "        Saves the processed dataframe in the db in a table named 'loaded_data'.\n",
    "        If the table with the same name already exsists then the function \n",
    "        replaces it.\n",
    "\n",
    "\n",
    "    SAMPLE USAGE\n",
    "        load_data_into_db()\n",
    "    '''\n",
    "    db_file_path = DB_PATH + DB_FILE_NAME\n",
    "\n",
    "    lead_scoring_raw_data_path = INFERENCE_SAMPLE_FILE_PATH\n",
    "    try:\n",
    "        db_conn = sqlite3.connect(db_file_path)\n",
    "        \n",
    "        print(f\"Connected to the database and started loading the data from @{lead_scoring_raw_data_path}\")\n",
    "\n",
    "        ls_data = pd.read_csv(lead_scoring_raw_data_path)\n",
    "\n",
    "        print(f\"Data Loaded\")\n",
    "\n",
    "        ls_data['total_leads_droppped'].fillna(0, inplace=True)\n",
    "        ls_data['referred_lead'].fillna(0, inplace=True)\n",
    "\n",
    "        ls_data.to_sql(name='loaded_data', con=db_conn, if_exists='replace', index=False)\n",
    "    except Error as e:\n",
    "            print(e)\n",
    "            return \"Error\"\n",
    "    finally:\n",
    "        if db_conn:\n",
    "            db_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9a7d4cf4-893e-4537-853d-e343e9382fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###############################################################################\n",
    "# Define function to map cities to their respective tiers\n",
    "###############################################################################\n",
    "\n",
    "def map_city_tier():\n",
    "    '''\n",
    "    This function maps all the cities to their respective tier as per the\n",
    "    mappings provided in the city_tier_mapping.py file. If a\n",
    "    particular city's tier isn't mapped(present) in the city_tier_mapping.py \n",
    "    file then the function maps that particular city to 3.0 which represents\n",
    "    tier-3.\n",
    "\n",
    "\n",
    "    INPUTS\n",
    "        DB_FILE_NAME : Name of the database file\n",
    "        DB_PATH : path where the db file should be\n",
    "        city_tier_mapping : a dictionary that maps the cities to their tier\n",
    "\n",
    "    \n",
    "    OUTPUT\n",
    "        Saves the processed dataframe in the db in a table named\n",
    "        'city_tier_mapped'. If the table with the same name already \n",
    "        exsists then the function replaces it.\n",
    "\n",
    "    \n",
    "    SAMPLE USAGE\n",
    "        map_city_tier()\n",
    "\n",
    "    '''\n",
    "    db_file_path = DB_PATH + DB_FILE_NAME\n",
    "\n",
    "    try:\n",
    "        db_conn = sqlite3.connect(db_file_path)\n",
    "\n",
    "        \"\"\" 1. Read the mapping information from the city_tier.py file\"\"\"\n",
    "\n",
    "        map_location = '/home/assignment/01_data_pipeline/notebooks/Maps/city_tier.py'\n",
    "\n",
    "        print(f\"Loading the Maps data from {map_location}\")\n",
    "        map_city_spec = importlib.util.spec_from_file_location('Maps', map_location)\n",
    "        Maps = importlib.util.module_from_spec(map_city_spec)\n",
    "        map_city_spec.loader.exec_module(Maps)\n",
    "\n",
    "        #from Maps.city_tier import city_tier_mapping\n",
    "        #Maps.city_tier.city_tier_mapping\n",
    "\n",
    "        \"\"\"#2. Read the data from the loaded_data table in database file location: DB_PATH + DB_FILE_NAME\"\"\"\n",
    "\n",
    "        print(f\"Read the data from the 'loaded_data' table in database file location: {db_file_path}\")\n",
    "        loaded_data = pd.read_sql('SELECT * FROM loaded_data', con=db_conn)\n",
    "\n",
    "        \"\"\"#3. Update the data of the loaded_data with the city_tier details mapped as per the Details.\"\"\"\n",
    "\n",
    "        print(f\"Update the data of the loaded_data with the city_tier details mapped as per the Details.\")\n",
    "\n",
    "        #city_tier_mapping = Maps.city_tier_mapping  #Module loaded dynamically as the abs. path is outside.\n",
    "\n",
    "        loaded_data[\"city_tier\"] = loaded_data[\"city_mapped\"].map(city_tier_mapping.city_tier_mapping)\n",
    "        loaded_data[\"city_tier\"] = loaded_data[\"city_tier\"].fillna(3.0)\n",
    "\n",
    "        #map_city.city_tier_mapping\n",
    "\n",
    "        \"\"\"#4. Save the information as a new table city_tier_mapped in the database.\"\"\"\n",
    "        \n",
    "        print(f\"Save the information as a new table city_tier_mapped in the database.\")\n",
    "        loaded_data.to_sql(name='city_tier_mapped', con=db_conn, if_exists='replace', index=False)\n",
    "\n",
    "    except Error as e:\n",
    "            print(e)\n",
    "            return \"Error\"\n",
    "    finally:\n",
    "        if db_conn:\n",
    "            db_conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d5a3e392-d04b-4c03-9455-65af899eca01",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################################\n",
    "# Define function to map insignificant categorial variables to \"others\"\n",
    "###############################################################################\n",
    "\n",
    "\n",
    "def map_categorical_vars():\n",
    "    '''\n",
    "    This function maps all the insignificant variables present in 'first_platform_c'\n",
    "    'first_utm_medium_c' and 'first_utm_source_c'. The list of significant variables\n",
    "    should be stored in a python file in the 'significant_categorical_level.py' \n",
    "    so that it can be imported as a variable in utils file.\n",
    "    \n",
    "\n",
    "    INPUTS\n",
    "        DB_FILE_NAME : Name of the database file\n",
    "        DB_PATH : path where the db file should be present\n",
    "        list_platform : list of all the significant platform.\n",
    "        list_medium : list of all the significat medium\n",
    "        list_source : list of all rhe significant source\n",
    "\n",
    "        **NOTE : list_platform, list_medium & list_source are all constants and\n",
    "                 must be stored in 'significant_categorical_level.py'\n",
    "                 file. The significant levels are calculated by taking top 90\n",
    "                 percentils of all the levels. For more information refer\n",
    "                 'data_cleaning.ipynb' notebook.\n",
    "  \n",
    "\n",
    "    OUTPUT\n",
    "        Saves the processed dataframe in the db in a table named\n",
    "        'categorical_variables_mapped'. If the table with the same name already \n",
    "        exsists then the function replaces it.\n",
    "\n",
    "    \n",
    "    SAMPLE USAGE\n",
    "        map_categorical_vars()\n",
    "    '''\n",
    "    db_file_path = DB_PATH + DB_FILE_NAME\n",
    "    list_platform = significant_categorical_level.list_platform\n",
    "    list_medium = significant_categorical_level.list_medium\n",
    "    list_source = significant_categorical_level.list_source\n",
    "    try:\n",
    "        db_conn = sqlite3.connect(db_file_path)\n",
    "\n",
    "        \"\"\"#1. Read the data from the city_tier_mapped table in database file location: DB_PATH + DB_FILE_NAME\"\"\"\n",
    "\n",
    "        print(f\"Read the data from the 'city_tier_mapped' table in database file location: {db_file_path}\")\n",
    "        loaded_data = pd.read_sql('SELECT * FROM city_tier_mapped', con=db_conn)\n",
    "\n",
    "        \"\"\"#2. Update the data of the city_tier_mapped with the 'first_platform_c', 'first_utm_medium_c' and 'first_utm_source_c' details mapped as per the Details.\"\"\"\n",
    "\n",
    "        print(f\"Update the data of the city_tier_mapped with the 'first_platform_c', 'first_utm_medium_c' and 'first_utm_source_c' details mapped as per the Details.\")\n",
    "\n",
    "        # all the levels below 90 percentage are assgined to a single level called others\n",
    "        new_df = loaded_data[~loaded_data['first_platform_c'].isin(list_platform)] # get rows for levels which are not present in list_platform\n",
    "        new_df['first_platform_c'] = \"others\" # replace the value of these levels to others\n",
    "        old_df = loaded_data[loaded_data['first_platform_c'].isin(list_platform)] # get rows for levels which are present in list_platform\n",
    "        df = pd.concat([new_df, old_df]) # concatenate new_df and old_df to get the final dataframe\n",
    "\n",
    "        # all the levels below 90 percentage are assgined to a single level called others\n",
    "        new_df = df[~df['first_utm_medium_c'].isin(list_medium)] # get rows for levels which are not present in list_medium\n",
    "        new_df['first_utm_medium_c'] = \"others\" # replace the value of these levels to others\n",
    "        old_df = df[df['first_utm_medium_c'].isin(list_medium)] # get rows for levels which are present in list_medium\n",
    "        df = pd.concat([new_df, old_df]) # concatenate new_df and old_df to get the final dataframe\n",
    "\n",
    "        # all the levels below 90 percentage are assgined to a single level called others\n",
    "        new_df = df[~df['first_utm_source_c'].isin(list_source)] # get rows for levels which are not present in list_source\n",
    "        new_df['first_utm_source_c'] = \"others\" # replace the value of these levels to others\n",
    "        old_df = df[df['first_utm_source_c'].isin(list_source)] # get rows for levels which are present in list_source\n",
    "        df = pd.concat([new_df, old_df]) # concatenate new_df and old_df to get the final dataframe\n",
    "\n",
    "        #map_city.city_tier_mapping\n",
    "\n",
    "        \"\"\"#4. Save the information as a new table categorical_variables_mapped in the database.\"\"\"\n",
    "        \n",
    "        print(f\"Save the information as a new table categorical_variables_mapped in the database.\")\n",
    "        df.to_sql(name='categorical_variables_mapped', con=db_conn, if_exists='replace', index=False)\n",
    "\n",
    "    except Error as e:\n",
    "            print(e)\n",
    "            return \"Error\"\n",
    "    finally:\n",
    "        if db_conn:\n",
    "            db_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "474be6b0-88f3-4b35-9d6a-00bd292cac39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##############################################################################\n",
    "# Define function that maps interaction columns into 4 types of interactions\n",
    "##############################################################################\n",
    "def interactions_mapping():\n",
    "    '''\n",
    "    This function maps the interaction columns into 4 unique interaction columns\n",
    "    These mappings are present in 'interaction_mapping.csv' file. \n",
    "\n",
    "\n",
    "    INPUTS\n",
    "        DB_FILE_NAME: Name of the database file\n",
    "        DB_PATH : path where the db file should be present\n",
    "        INTERACTION_MAPPING : path to the csv file containing interaction's\n",
    "                                   mappings\n",
    "        INDEX_COLUMNS_TRAINING : list of columns to be used as index while pivoting and\n",
    "                                 unpivoting during training\n",
    "        INDEX_COLUMNS_INFERENCE: list of columns to be used as index while pivoting and\n",
    "                                 unpivoting during inference\n",
    "        NOT_FEATURES: Features which have less significance and needs to be dropped\n",
    "                                 \n",
    "        NOTE : Since while inference we will not have 'app_complete_flag' which is\n",
    "        our label, we will have to exculde it from our features list. It is recommended \n",
    "        that you use an if loop and check if 'app_complete_flag' is present in \n",
    "        'categorical_variables_mapped' table and if it is present pass a list with \n",
    "        'app_complete_flag' column, or else pass a list without 'app_complete_flag'\n",
    "        column.\n",
    "\n",
    "    \n",
    "    OUTPUT\n",
    "        Saves the processed dataframe in the db in a table named \n",
    "        'interactions_mapped'. If the table with the same name already exsists then \n",
    "        the function replaces it.\n",
    "        \n",
    "        It also drops all the features that are not requried for training model and \n",
    "        writes it in a table named 'model_input'\n",
    "\n",
    "    \n",
    "    SAMPLE USAGE\n",
    "        interactions_mapping()\n",
    "    '''\n",
    "    \n",
    "    db_file_path = DB_PATH + DB_FILE_NAME\n",
    "    INTERACTION_MAPPING = data_cleaning_constants.INTERACTION_MAPPING\n",
    "    INDEX_COLUMNS_TRAINING = data_cleaning_constants.INDEX_COLUMNS_TRAINING\n",
    "    try:\n",
    "        db_conn = sqlite3.connect(db_file_path)\n",
    "\n",
    "        \"\"\" 1. Read the mapping information from the interaction_mapping.csv file\"\"\"\n",
    "\n",
    "        print(f\"Loading the Maps data from {INTERACTION_MAPPING}\")\n",
    "        # read the interaction mapping file\n",
    "        df_event_mapping = pd.read_csv(INTERACTION_MAPPING, index_col=[0])\n",
    "\n",
    "        #from Maps.city_tier import city_tier_mapping\n",
    "        #Maps.city_tier.city_tier_mapping\n",
    "\n",
    "        \"\"\"#2. Read the data from the categorical_variables_mapped table in database file location: DB_PATH + DB_FILE_NAME\"\"\"\n",
    "\n",
    "        print(f\"Read the data from the 'categorical_variables_mapped' table in database file location: {db_file_path}\")\n",
    "        loaded_data = pd.read_sql('SELECT * FROM categorical_variables_mapped', con=db_conn)\n",
    "\n",
    "        \"\"\"#3. Update the data of the categorical_variables_mapped with the interaction mapping data.\"\"\"\n",
    "\n",
    "        print(f\"Update the data of the categorical_variables_mapped with the interaction mapping data.\")\n",
    "\n",
    "        index_columns_training = INDEX_COLUMNS_TRAINING\n",
    "\n",
    "        if 'app_complete_flag' in loaded_data.columns:\n",
    "            index_columns_training.append('app_complete_flag')\n",
    "\n",
    "        # unpivot the interaction columns and put the values in rows\n",
    "        df_unpivot = pd.melt(loaded_data, id_vars=index_columns_training, var_name='interaction_type', value_name='interaction_value')\n",
    "\n",
    "        df_unpivot['interaction_value'] = df_unpivot['interaction_value'].fillna(0)\n",
    "\n",
    "        # map interaction type column with the mapping file to get interaction mapping\n",
    "        df = pd.merge(df_unpivot, df_event_mapping, on='interaction_type', how='left')\n",
    "\n",
    "        #dropping the interaction type column as it is not needed\n",
    "        df = df.drop(['interaction_type'], axis=1)\n",
    "\n",
    "        # pivoting the interaction mapping column values to individual columns in the dataset\n",
    "        df_pivot = df.pivot_table(\n",
    "                values='interaction_value', index=index_columns_training, columns='interaction_mapping', aggfunc='sum')\n",
    "        df_pivot = df_pivot.reset_index()\n",
    "\n",
    "        # the rows are again converted back to columns. 37 rows gets converted to a single row.\n",
    "        print(f\"Shape of the dataset: {df_pivot.shape}\")\n",
    "\n",
    "\n",
    "        \"\"\"#4. Save the information as a new table model_input in the database.\"\"\"\n",
    "        \n",
    "        print(f\"Save the information as a new table model_input in the database.\")\n",
    "        df_pivot.to_sql(name='model_input', con=db_conn, if_exists='replace', index=False)\n",
    "\n",
    "    except Error as e:\n",
    "            print(e)\n",
    "            return \"Error\"\n",
    "    finally:\n",
    "        if db_conn:\n",
    "            db_conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d6247c5-0382-4aad-b040-2f6babab578b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database and started loading the data from @/home/assignment/03_inference_pipeline/data/leadscoring_inference.csv\n",
      "Data Loaded\n"
     ]
    }
   ],
   "source": [
    "load_data_into_db()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "611f9058-12d2-4450-be28-0914d52d07cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Maps data from /home/assignment/01_data_pipeline/notebooks/Maps/city_tier.py\n",
      "Read the data from the 'loaded_data' table in database file location: /home/Databases/lead_scoring_data_cleaning.db\n",
      "Update the data of the loaded_data with the city_tier details mapped as per the Details.\n",
      "Save the information as a new table city_tier_mapped in the database.\n"
     ]
    }
   ],
   "source": [
    "map_city_tier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b604ba88-8d62-45c6-82a5-25bcf350f3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read the data from the 'city_tier_mapped' table in database file location: /home/Databases/lead_scoring_data_cleaning.db\n",
      "Update the data of the city_tier_mapped with the 'first_platform_c', 'first_utm_medium_c' and 'first_utm_source_c' details mapped as per the Details.\n",
      "Save the information as a new table categorical_variables_mapped in the database.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_13222/963806677.py:55: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['first_platform_c'] = \"others\" # replace the value of these levels to others\n",
      "/tmp/ipykernel_13222/963806677.py:61: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['first_utm_medium_c'] = \"others\" # replace the value of these levels to others\n",
      "/tmp/ipykernel_13222/963806677.py:67: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_df['first_utm_source_c'] = \"others\" # replace the value of these levels to others\n"
     ]
    }
   ],
   "source": [
    "map_categorical_vars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f5b177f0-6547-452f-8244-b619d6b6e598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the Maps data from /home/assignment/01_data_pipeline/scripts/mapping/interaction_mapping.csv\n",
      "Read the data from the 'categorical_variables_mapped' table in database file location: /home/Databases/lead_scoring_data_cleaning.db\n",
      "Update the data of the categorical_variables_mapped with the interaction mapping data.\n",
      "Shape of the dataset: (24976, 12)\n",
      "Save the information as a new table model_input in the database.\n"
     ]
    }
   ],
   "source": [
    "interactions_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d98d3c-5f50-45da-995c-8a9476a00f4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7cc4b115-b8da-45d3-9bc6-5a525a9b2f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the database /home/Databases/lead_scoring_data_cleaning.db\n",
      "Reading from file is Complete!\n"
     ]
    }
   ],
   "source": [
    "encode_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b67d9de-1c29-41c0-b26b-1b476d9e807a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the database /home/Databases/lead_scoring_data_cleaning.db\n"
     ]
    }
   ],
   "source": [
    "input_features_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e53b01ee-1502-4ccd-a3ac-b3312f8c0827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the database /home/Databases/lead_scoring_data_cleaning.db\n",
      "Reading features from the database /home/Databases/lead_scoring_data_cleaning.db\n"
     ]
    }
   ],
   "source": [
    "load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6d8c805a-b636-43a4-9398-9356e607023a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to the database /home/Databases/lead_scoring_data_cleaning.db\n"
     ]
    }
   ],
   "source": [
    "prediction_col_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21efe536-36a1-4f0e-a6c3-c15cb727c817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.isfile('/home/airflow/dags/Lead_scoring_inference_pipeline/prediction_distribution.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6642b4e5-7bbb-4a10-8d91-a65783dd47cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
